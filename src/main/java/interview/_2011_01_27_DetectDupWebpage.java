package interview;

/*
 * input : 10억개의 url, 그리고 각 url 에 해당하는 web page ( very large )
 * find : 내용이 중복되는 웹페이지들을 찾기.
 */
public class _2011_01_27_DetectDupWebpage {

}

/*
 * 10억개에 쫄지말자. 작은 경우부터 시작하면 된다.
 * 우선 두개의 페이지가 중복된다는 것을 어떻게 알 것인가?
 * 페이지 내용이 매우 크기 때문에 md5 같은 것을 이용하여 message digest를 해야한다.
 * 해싱이 완벽하다고 가정하면(즉 no collision) 해시값이 같으면 페이지가 중복되는 것이다.
 * 자 이제 작은 개수의 url에 대해 생각해보자.
 * 우리는 각 url에 대한 해시값을 가지고 있다.
 * 한 페이지가 다른 페이지와 중복되는지를 살펴보기 위해 모든 페이지의 해시값을 살펴본다면
 * 전체 성능은 O(n^2) 이 된다.
 * 트리를 이용하여 해시값이 트리에 없으면 삽입하고 트리에 이미 존재하면 중복되는 것으로 판단해도 된다.
 * 이럴 경우 전체 성능은 O(n logn) 이 된다.
 * 이번엔 해시 테이블을 이용해보자.
 * 해시값을 또다시 해싱하여 해시테이블에 저장하면 
 * 삽입,삭제 연산이 O(1)인 해시테이블의 특징상
 * 전체 성능은 O(n)이 된다.
 * 
 * 자 이제 10억개의 url에 대해 생각해보자.
 * 페이지 수가 매우 많기 때문에 해시 테이블을 한 머신의 메모리에 모두 로드할 수 없다.
 * 그러면 취할 수 있는 방법은 두가지이다.
 * 1.파일을 이용한다.
 * 2.여러대의 머신을 이용한다.
 * 
 * 2를 쓰면 병렬처리도 가능하고 IO로 인한 오버헤드가 없기때문에 ( 대신 머신간 점프로 인한 오버헤드가 있다 ) 더 좋다.
 * 이 문제에서는 별도의 명시적인 메모리 제한은 없으므로 2번 방법을 쓰도록 하자.
 * 우선 페이지의 해시값을 해싱할 때, 이것이 어떤 머신으로 가야 하는지 정해줘야 한다.
 * 머신이 n개라면 해시값%n 으로 어떤 머신으로 갈지 정해주면 된다.
 * 그리고 나서 하나의 머신에서 또다시 해싱하면 된다.
 * 
 * 그렇다면 실제로 n이 얼마나 되야할지 적절한 가정들을 통해 알아보자.
 * 사실 구현을 어떤식으로 하느냐에 따라 다른데,
 * 중복이 될 경우는 어떤 url과 중복되었다고 알려준다고 해보자.
 * 해시테이블의 key는 md5 값이고, value는 url이 된다.
 * key를 4 byte라고 하고
 * url의 평균길이를 30byte 라고 하자. ( google.com 의 3배를 한 것이다 )
 * 한 머신에서 쓸수 있는 메모리를 1GB 라고 하면
 * 한 머신에서의 해시테이블의 최대 엔트리 수는 1GB/34byte = 2900만 정도가 된다.
 * 따라서 10억/2900만 = 35대 정도의 머신이 필요하다.
 * 
 */
